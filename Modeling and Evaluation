#Feature Selection using different techniques

#First Technique: F-statistic to select top features
#repeat this part for different values of k (19, 16, 13, and 10 features) 
selector = SelectKBest(score_func=f_classif, k=22)
X_train_selected_f = selector.fit_transform(X_train_resampled, y_train_resampled)

selected_features_f = X_train_resampled.columns[selector.get_support()]
print("Selected Features using F-statistic:", selected_features_f.tolist())

#transform dataframe with selected features
X_train_f = pd.DataFrame(X_train_selected_f, columns=selected_features_f)
X_test_f = X_test[selected_features_f]

#Second Technique: chi-square to select top features
#repeat this part for different values of k (19, 16, 13, and 10 features) 
selector = SelectKBest(score_func=chi2, k=22)
X_train_selected_chi = selector.fit_transform(X_train_resampled, y_train_resampled)

selected_features_chi = X_train_resampled.columns[selector.get_support()]
print("Selected Features using chi-square:", selected_features.tolist())

#transform dataframe with selected features
X_train_chi = pd.DataFrame(X_train_selected_chi, columns=selected_features_chi)
X_test_chi = X_test[selected_features_chi]

#Third Technique: RFE to select top features
#repeat this part for different values of k (19, 16, 13, and 10 features) 
base_model = RandomForestClassifier(random_state=42)
rfe = RFE(estimator=base_model, n_features_to_select=22, step=1)
rfe.fit(X_train_resampled, y_train_resampled)

#get the selected features
selected_features = X_train_resampled.columns[rfe.support_]
print("Selected Features using RFE:", selected_features.tolist())

#transform dataframe with selected features
X_train_selected = X_train_resampled[selected_features]
X_test_selected = X_test[selected_features]

#Fourth Technique: MI to select top features
#repeat this part for different values of k (19, 16, 13, and 10 features) 
mi_scores = mutual_info_classif(X_train_resampled, y_train_resampled, random_state=42)
mi_df = pd.DataFrame({
   'Feature': X_train_resampled.columns,
    'MI Score': mi_scores
}).sort_values(by='MI Score', ascending=False)

print("Mutual Information Scores:", mi_df)

top_features = mi_df.nlargest(22, 'MI Score')['Feature'].tolist()
print("Top Features via MI:", top_features)

#transform dataframe with selected features
X_train_mi = X_train_resampled[top_features_mi]
X_test_mi = X_test[top_features_mi]

#training models

#LR algorithm
lr_model = LogisticRegression(random_state=42, max_iter=1000)

#RF algorithm
rf_model  =  RandomForestClassifier(    n_estimators=200,          # Number of trees
    max_depth=10,              # Max depth of trees
    min_samples_split=5,       # Min samples to split a node
    min_samples_leaf=4,        # Min samples at a leaf
    max_features='sqrt',       # Number of features to consider
    bootstrap=True,            # Use bootstrap samples
    n_jobs=-1,                 # Use all CPUs for parallel processing
    random_state=42,           # Ensure reproducibility
    oob_score=True             # Estimate out-of-bag score
)

#SVM
svm_model  =  SVC(probability=True,kernel='rbf', random_state=42)

#Multilayer Perceptron Classifier (MLP)
mlp_model  = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=200)

#K-Nearest Neighbors Classifier (KNN)
knn_model  = KNeighborsClassifier(n_neighbors=5)

#Gradient Boosting Classifier (GB)
gbc_model  = GradientBoostingClassifier(
    n_estimators=200,             # Number of trees
    learning_rate=0.05,           # Learning rate
    max_depth=5,                  # Max depth of trees
    min_samples_split=5,          # Min samples to split a node
    min_samples_leaf=3,           # Min samples at a leaf
    subsample=0.8,                # Subsample fraction
    max_features='sqrt',          # Max features to consider
    random_state=42,              # Ensure reproducibility
    )

#XGBoost Classifier
xgb_model = XGBClassifier(
    n_estimators=200,
    learning_rate=0.05,
    max_depth=5,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

#Catboost
cb_model = CatBoostClassifier(verbose=0, random_state=42)

#Decision Tree Classifier
dt_model = DecisionTreeClassifier(
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=4,
    random_state=42
)

#Linear Discriminant Analysis (LDA)
lda_model = LDA()

# Gaussian Naive Bayes (GNB)
gnb_model = GaussianNB()

#Evaluation (apply ach feature selection technique separately)
metricslr_reduced = trainandtest('LR', X_train_resampled, y_train_resampled, X_test, y_test, lr_model)
metricsrf_reduced = trainandtest('RF', X_train_resampled, y_train_resampled, X_test, y_test, rf_model)
metricssvm_reduced = trainandtest('SVM', X_train_resampled, y_train_resampled, X_test, y_test, svm_model)
metricsmlp_reduced = trainandtest('MLP', X_train_resampled, y_train_resampled, X_test, y_test, mlp_model)
metricsknn_reduced = trainandtest('KNN', X_train_resampled, y_train_resampled, X_test, y_test, knn_model)
metricsgbc_reduced = trainandtest('GBC', X_train_resampled, y_train_resampled, X_test, y_test, gbc_model)
metricsxgb_reduced = trainandtest('XGB', X_train_resampled, y_train_resampled, X_test, y_test, xgb_model)
metricscb_reduced = trainandtest('CB', X_train_resampled, y_train_resampled, X_test, y_test, cb_model)
metricsdt_reduced = trainandtest('DT', X_train_resampled, y_train_resampled, X_test, y_test, dt_model)
metricslda_reduced = trainandtest('LDA', X_train_resampled, y_train_resampled, X_test, y_test, lda_model)
metricsgnb_reduced = trainandtest('GNB', X_train_resampled, y_train_resampled, X_test, y_test, gnb_model)

# Combine models using VotingClassifier
voting_clf = VotingClassifier(estimators=[
   ('CB', cb_model),
   ('XGB', xgb_model),
   ('GBC', gbc_model),
   ('KNN', knn_model),
], voting='soft')

metricsvoting_reduced = trainandtest('VOTING', X_train_resampled, y_train_resampled, X_test, y_test, voting_clf)

metricsvoting_reduced
